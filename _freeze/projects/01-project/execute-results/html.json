{
  "hash": "036ec4f8b474779116caf183846274f4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: College Expense Prediction (Code Displayed and Interpreted) \nsubtitle: This project predicts yearly US college attendance costs and if the school is private or not using factors like out-of-state enrollment, and student population, with a random forest model performing best. I used tree plots, bagging models, and random forest models. Also, I discuss what I learned from the model outputs.\n\nimages: image/college-expense.jpeg\ntoc: false\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCollege <- read.csv(\"CollegeF23.csv\")\nset.seed(11281975)\nindex =sample(nrow(College), 2100,replace = FALSE)\nC.train=College[index,]\nC.test=College[-index,]\nhead(College)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Private Apps Accept Enroll Top10perc Top25perc F.Undergrad P.Undergrad\n1     Yes  996    866    377        29        58        1411          72\n2     Yes  548    428    167        18        46         618         113\n3     Yes  450    430    125        20        46         488          43\n4     Yes 1470   1199    425        21        76        1820         558\n5     Yes 1465    810    313        71        95        1088          16\n6     Yes  417    349    137        60        89         510          63\n  Outstate Room.Board Books Personal PhD Terminal S.F.Ratio perc.alumni\n1    12065       3615   430      685  62       78      12.5          41\n2     8958       3670   300     1000  53       59      15.3          26\n3     9950       3920   300     1300  76       76      11.8          25\n4    11040       4840   400      900  89       92      13.3          28\n5    18165       6750   500     1200 100      100      12.3          49\n6    12960       5450   450      875  92       97       7.7          37\n  Grad.Rate Expend\n1        80   8596\n2        64   9798\n3        47   9466\n4        94   8118\n5        89  17449\n6        59  19016\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nUniqueCount <- sapply(College, function(x) length(unique(x)))\nFirstFewValues <- sapply(College, function(x) paste0(head(x, 5), collapse = \", \"))\n\ndf.unique <- data.frame(\n  Column = names(UniqueCount),\n  UniqueCount = UniqueCount,\n  FirstFewValues = FirstFewValues,\n  row.names = NULL\n)\n\ndf.unique\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Column UniqueCount                  FirstFewValues\n1      Private           2         Yes, Yes, Yes, Yes, Yes\n2         Apps         617       996, 548, 450, 1470, 1465\n3       Accept         605        866, 428, 430, 1199, 810\n4       Enroll         518         377, 167, 125, 425, 313\n5    Top10perc          80              29, 18, 20, 21, 71\n6    Top25perc          89              58, 46, 46, 76, 95\n7  F.Undergrad         619      1411, 618, 488, 1820, 1088\n8  P.Undergrad         507            72, 113, 43, 558, 16\n9     Outstate         568 12065, 8958, 9950, 11040, 18165\n10  Room.Board         487    3615, 3670, 3920, 4840, 6750\n11       Books         108         430, 300, 300, 400, 500\n12    Personal         268      685, 1000, 1300, 900, 1200\n13         PhD          77             62, 53, 76, 89, 100\n14    Terminal          63             78, 59, 76, 92, 100\n15   S.F.Ratio         167    12.5, 15.3, 11.8, 13.3, 12.3\n16 perc.alumni          61              41, 26, 25, 28, 49\n17   Grad.Rate          79              80, 64, 47, 94, 89\n18      Expend         650   8596, 9798, 9466, 8118, 17449\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndim(C.train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2100   18\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(C.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 900  18\n```\n\n\n:::\n\n```{.r .cell-code}\nsum(is.na(College))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n\n\nThere are 2100 training data points and 900 testing data points, each with 18 predictors, and no NA values.\n\n## Here, we'll use a classification tree to find the most important variables in predicting the response (Private Status, in this case)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC.train$Private <- as.factor(C.train$Private)\nlevels(C.test$Private) <- levels(C.train$Private)\ntree_model <- tree(Private ~., data = C.train)\nsummary(tree_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification tree:\ntree(formula = Private ~ ., data = C.train)\nVariables actually used in tree construction:\n[1] \"F.Undergrad\" \"Outstate\"    \"Terminal\"    \"Top10perc\"   \"Grad.Rate\"  \n[6] \"PhD\"         \"Top25perc\"   \"Expend\"      \"Accept\"     \nNumber of terminal nodes:  17 \nResidual mean deviance:  0.1106 = 230.3 / 2083 \nMisclassification error rate: 0.01762 = 37 / 2100 \n```\n\n\n:::\n:::\n\n\n\n\n\n9 predictors used in the tree model: \n- \"F.Undergrad\" (Number of full-time undergraduates)\n- \"Outstate\"\n- \"Terminal\" (Percent of faculty with terminal degree)\n- \"Top10perc\" (New students from top 10% of high school class)\n- \"Grad.Rate\"\n- \"PhD\"\n- \"Top25perc\"\n- \"Expend\"\n- \"Accept\" (Number of applicants accepted)\n\nThese variables are what the classification tree identified as most important in predicting if the school is private or not. \n\n17 terminal nodes means the tree splits the data into 17 distinct groups or \"leaves\" based on the values of the predictor variables. This amount makes for an interpretable model. \n\nMisclassification Rate (MCR) of 17.62% is the percentage that the model did not predict correctly.\n\n## Make predictions with the testing data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(tree_model, C.test, type = \"class\")\ntable(C.test$Private, test_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     test_predictions\n       No Yes\n  No  236   8\n  Yes   4 652\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(4+8)/(236 + 8 + 4 + 652)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01333333\n```\n\n\n:::\n:::\n\n\n\n\nThe testing MCR for the tree model is 1.33%.\n\n\n## Find the optimal size of the tree to find the balance between simplicity and accuracy\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reduce the number of terminal nodes (pruning)\ncv_tree <- cv.tree(tree_model, FUN = prune.misclass)\nplot(cv_tree$size, cv_tree$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\nOptimal Size: 15\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune_model <- prune.misclass(tree_model, best = 15)\npar(mar = c(1, 1, 1, 1))  \nplot(prune_model, cex = 1.2)\ntext(prune_model, pretty = 100, cex = 0.5)  \n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\nTree models are basically just a yes/no decision tree but with an entire dataset! Now you can see how the dataset is split into subsets based on the values of predictors to predict the response. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(prune_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification tree:\nsnip.tree(tree = tree_model, nodes = c(52L, 12L))\nVariables actually used in tree construction:\n[1] \"F.Undergrad\" \"Outstate\"    \"Terminal\"    \"Top10perc\"   \"Grad.Rate\"  \n[6] \"PhD\"         \"Top25perc\"   \"Expend\"      \"Accept\"     \nNumber of terminal nodes:  15 \nResidual mean deviance:  0.1375 = 286.7 / 2085 \nMisclassification error rate: 0.0181 = 38 / 2100 \n```\n\n\n:::\n:::\n\n\n\n\nThe MCR of the pruned model is 18.1%. The MCR is slightly higher here than with 17 terminal nodes, and it illustrates the trade off between simplicity and accuracy. \n\n## Make predictions with the pruned model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(prune_model, newdata = C.test,type = \"class\")\ntable(test_predictions, C.test$Private)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                \ntest_predictions  No Yes\n             No  236   7\n             Yes   8 649\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(8 + 12)/(236 + 12 + 8 + 644)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.02222222\n```\n\n\n:::\n:::\n\n\n\n\nThe MCR is 2.22%. This is a huge improvement!\n\n## Moving on to a different response variable: Expend (College Attendance Expenditures)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(C.train,aes(x = Expend))+geom_density(fill = \"blue\", alpha = 0.5)+labs(title = \"Density Plot of Expend\",x = \"Expend\",y = \"Density\")+theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\nThe highest percentage of people are spending around $8K for college per year. \n\n## Do a log transformation on the response\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC.train$Expend <-log(C.train$Expend)\nC.test$Expend <-log(C.test$Expend)\n```\n:::\n\n\n\n\n\n## Create a tree model to predict log(Expend)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_tree_model <- tree(Expend ~ ., data = C.train)\nsummary(log_tree_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\ntree(formula = Expend ~ ., data = C.train)\nVariables actually used in tree construction:\n[1] \"Outstate\"  \"S.F.Ratio\" \"Enroll\"    \"Top10perc\" \"Top25perc\"\nNumber of terminal nodes:  11 \nResidual mean deviance:  0.04209 = 87.93 / 2089 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.6196 -0.1252  0.0000  0.0000  0.1086  1.1040 \n```\n\n\n:::\n:::\n\n\n\n\n\n7 predictors used for the model: \n- \"Outstate\"\n- \"S.F.Ratio\" \n- \"F.Undergrad\"\n- \"PhD\"\n- \"Room.Board\"\n- \"Top10perc\" \n- \"Top25perc\"\n\n14 terminal nodes\n\n## Make predictions and find MSE's for training and testing data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_predictions <- predict(log_tree_model, data = C.train)\ntrain_residuals <- C.train$Expend - train_predictions\nmean(train_residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04187042\n```\n\n\n:::\n:::\n\n\n\n\n\nTraining MSE:  0.0148%\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC.test$Private <- as.factor(C.test$Private)\ntest_predictions <- predict(log_tree_model, newdata = C.test)\ntest_residuals <- C.test$Expend - test_predictions\nmse_test <- mean(test_residuals^2)\nmse_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04207581\n```\n\n\n:::\n:::\n\n\n\n\nTesting MSE: 0.0146%\n\n## Find the optimal size\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_tree_2 <- cv.tree(log_tree_model)\nplot(cv_tree_2$size, cv_tree_2$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\nOptimal size: 14\n\n## Plot the pruned tree model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune_log_model <- prune.tree(log_tree_model, best = 14)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in prune.tree(log_tree_model, best = 14): best is bigger than tree size\n```\n\n\n:::\n\n```{.r .cell-code}\npar(mar = c(1, 1, 1, 1))  \nplot(prune_log_model, cex = 1.2)\ntext(prune_log_model, pretty = 10, cex = 0.7)\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n## Find predictions and MSE's\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_predictions <- predict(prune_log_model, data = C.train)\ntrain_residuals <- C.train$Expend - train_predictions\nmean(train_residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04187042\n```\n\n\n:::\n:::\n\n\n\n\nTraining MSE of pruned tree model: 0.0144%\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(prune_log_model, newdata = C.test)\ntest_residuals <- C.test$Expend - test_predictions\nmean(test_residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04207581\n```\n\n\n:::\n:::\n\n\n\n\nTesting MSE: 0.0146%\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(cor(train_predictions, C.train$Expend))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.751356\n```\n\n\n:::\n:::\n\n\n\n\nThe r squared of the training data of 75.14%\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(cor(test_predictions, C.test$Expend))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7460448\n```\n\n\n:::\n:::\n\n\n\n\nThe r squared of the testing data is 74.6%\n\nBoth R^2 values are decently high, so around 75% of the variation in the data is explained by the model.\n\n## Now, we'll create a bagging model\nBagging involves training multiple copies of the same model on different subsets of the data, and this helps to reduce overfitting, it makes the model more robust, and it can give a more stable and accurate prediction than a single model. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nC.train$Private <- as.factor(C.train$Private)\nbagging <- randomForest(Private ~., data = C.train, mtry = 17, importance = TRUE) # mtrry = 17 because that is the total number of features\nbagging\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Private ~ ., data = C.train, mtry = 17,      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 17\n\n        OOB estimate of  error rate: 0.67%\nConfusion matrix:\n     No  Yes class.error\nNo  498    9 0.017751479\nYes   5 1588 0.003138732\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(bagging)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   No      Yes MeanDecreaseAccuracy MeanDecreaseGini\nApps         7.150819 14.41445             14.90384         7.321799\nAccept      14.522374 15.68410             20.86226        18.216586\nEnroll       9.302385 11.10371             14.09062         9.824299\nTop10perc   24.896203 21.10477             27.42748        14.821269\nTop25perc   21.360739 26.07698             26.53676        14.772428\nF.Undergrad 79.770776 74.77521            111.07985       379.391490\nP.Undergrad 21.144146 36.01142             33.76705        28.318235\nOutstate    96.825397 99.07068            133.88093       210.100275\nRoom.Board  14.409165 36.61628             37.74531        16.282566\nBooks       14.510306 21.76875             21.08725         5.435088\nPersonal    10.079723 15.52761             16.73388         3.675347\nPhD         22.107278 19.47482             26.88305        14.680158\nTerminal    36.187643 24.12532             40.60947        19.099854\nS.F.Ratio   14.633135 16.63305             19.78574         6.582126\nperc.alumni 25.413291 18.71552             28.23599         7.142642\nGrad.Rate   17.076210 15.63929             19.98084         8.486667\nExpend       9.248024 15.76433             17.35448         6.377506\n```\n\n\n:::\n:::\n\n\n\n\n- In the first 2 columns, higher values reveal that the variable was more useful in differentiating between \"No\" (not private) and \"Yes\" (private).\n- MeanDecreaseAccuracy quantifies how much the modelâ€™s accuracy decreases when the values of a specific variable are made \"less useful\" while keeping all other variables the same. Books has a lower MeanDecreaseAccuracy (21.09), so it's less critical for accurate predictions.\n- Variables with a higher MeanDecreaseGini are more important for splitting the data into homogenous groups such as distinguishing between private and not-private. F.Undergrad (379.39) is key for keeping the nodes pure in the trees.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importance\nvarImpPlot(bagging)\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\nFrom the plot, the most important variable is \"Outstate\" based on MeanDecreaseAccuracy.\n\n## Make predictions and find the MCR\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n# Report testing MCR\npredictions <- predict(bagging, newdata = C.test)\nactual <- C.test$Private  \nmisclassification_rate <- mean(predictions != actual)\nmisclassification_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001111111\n```\n\n\n:::\n:::\n\n\n\n\nThe testing MCR is 0.11%\n \n## Create a random forest model on the response variable \"Private\" with mtry = 4 (less features than bagging)\nIn Bagging, each tree would consider all features to choose the best split at each node. In Random Forests, each tree only considers a random subset of features at each split, leading to extra randomness. This randomness makes the trees different and reduces correlation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nrf <- randomForest(Private ~., data = C.train, mtry = 4, importance = TRUE)\nrf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Private ~ ., data = C.train, mtry = 4,      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 0.57%\nConfusion matrix:\n     No  Yes class.error\nNo  498    9 0.017751479\nYes   3 1590 0.001883239\n```\n\n\n:::\n:::\n\n\n\n\n## Make predictions and find the MCR\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Report testing MCR\npredictions.2 <- predict(rf, newdata = C.test)\nmisclassification_rate <- mean(predictions.2 != C.test$Private)\nmisclassification_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.003333333\n```\n\n\n:::\n:::\n\n\n\n\nThe testing MCR is 0.33%\n\n## Try out a bagging model on the \"Expend\" response variable\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nlog_bagging <- randomForest(Expend ~., data = C.train, mtry = 17, importance = TRUE)\nlog_bagging\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Expend ~ ., data = C.train, mtry = 17,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 17\n\n          Mean of squared residuals: 0.003716069\n                    % Var explained: 97.79\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(log_bagging)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               %IncMSE IncNodePurity\nPrivate       7.010438     0.1513677\nApps         29.972199     3.9449379\nAccept       22.662350     3.0799850\nEnroll       26.111518     3.5902980\nTop10perc    69.167323    28.7649012\nTop25perc    23.935746     7.5878241\nF.Undergrad  29.544402     3.4305256\nP.Undergrad  60.989019     6.8355690\nOutstate    130.163262   192.1074146\nRoom.Board   33.336093    10.0973513\nBooks        39.603263     3.7910810\nPersonal     43.951925     4.6812399\nPhD          51.789758    14.4131545\nTerminal     37.400131     7.5715491\nS.F.Ratio   134.649010    52.4663203\nperc.alumni  46.988899     5.4711661\nGrad.Rate    27.815089     4.6038497\n```\n\n\n:::\n:::\n\n\n\n\nThe importance plot looks a little different here because of the nature of the response variable \"expend\". 'Outstate' (126.625) and student-faculty ratio 'S.F.Ratio' (134.729) have the highest %IncMSE, showing that they significantly reduce prediction error. These variables also have high IncNodePurity, showing they contribute heavily to reducing node variance.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importance\nvarImpPlot(log_bagging)\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n\n\n\nS.F ratio is the most important in terms of impacting the MSe and Outstate reduces node variance the most.\n\n## Make predictions and find MSE\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions.2 <- predict(log_bagging, newdata = C.test)\ntest_residuals <- C.test$Expend - test_predictions.2\nmean(test_residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002340102\n```\n\n\n:::\n:::\n\n\n\n\nThe testing MSE 0.0000923%\n\nThe R squared of the model is 97.73%. So almost 98% of the variation in the data is explained by the model!\n\n## Test a random forest with the response \"Expend\"\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nlog_bagging_4 <-randomForest(Expend~., data = C.train, mtry = 4, importance = TRUE)\nlog_bagging_4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Expend ~ ., data = C.train, mtry = 4,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 0.00359694\n                    % Var explained: 97.86\n```\n\n\n:::\n:::\n\n\n\n\n\n## Make predictions and find the MSE\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions.3 <- predict(log_bagging_4, newdata = C.test)\nmean((predictions.3 - C.test$Expend)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002163567\n```\n\n\n:::\n:::\n\n\n\n\nThe MSE is 0.0000870%\n\nThe R squared is 97.7%\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC.test$pred <- predictions.3\nggplot(C.test,aes(x = Expend, y = predictions.3))+geom_point(color = \"green\", alpha = 0.6)+ # Scatter plot\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") + # 45-degree line\n  labs(title = \"Actual vs Predicted log(Expend)\",x = \"Actual log(Expend)\",y = \"Predicted log(Expend)\")+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n\nThe plot shows a linear relationship in the actual and the predicted data, and the predicted data matches the actual data really well.",
    "supporting": [
      "01-project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}