{
  "hash": "942ce1ac1ab9fe206f6a33093fe5f0e3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: College Expense Prediction\nsubtitle: This project uses a dataset that includes factors about different colleges, such as if they are private, how many out of state students there are, and how many students the college has. The factors are used to predict the cost of attending the school. In the project, different models were fit to the data, and the project shows that a random forest model performed the best. \nimages: image/college-expense.jpeg\ntoc: false\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nCollege <- read.csv(\"CollegeF23.csv\")\nset.seed(11281975)\nindex =sample(nrow(College), 2100,replace = FALSE)\nC.train=College[index,]\nC.test=College[-index,]\ndim(C.train)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2100   18\n```\n\n\n:::\n\n```{.r .cell-code}\ndim(C.test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 900  18\n```\n\n\n:::\n:::\n\n\n\n\n\n# Q1 \n\n# A\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(C.train$Private)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  No  Yes \n 507 1593 \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(C.test$Private)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n No Yes \n244 656 \n```\n\n\n:::\n\n```{.r .cell-code}\nprop.table(table(C.train$Private))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n       No       Yes \n0.2414286 0.7585714 \n```\n\n\n:::\n\n```{.r .cell-code}\nprop.table(table(C.test$Private))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n       No       Yes \n0.2711111 0.7288889 \n```\n\n\n:::\n:::\n\n\n\n\n# B\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC.train$Private <- as.factor(C.train$Private)\nlevels(C.test$Private) <- levels(C.train$Private)\ntree_model <- tree(Private ~., data = C.train)\nsummary(tree_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification tree:\ntree(formula = Private ~ ., data = C.train)\nVariables actually used in tree construction:\n[1] \"F.Undergrad\" \"Outstate\"    \"Terminal\"    \"Top10perc\"   \"Grad.Rate\"  \n[6] \"PhD\"         \"Top25perc\"   \"Expend\"      \"Accept\"     \nNumber of terminal nodes:  17 \nResidual mean deviance:  0.1106 = 230.3 / 2083 \nMisclassification error rate: 0.01762 = 37 / 2100 \n```\n\n\n:::\n:::\n\n\n\n\n## i. \n9 predictors used: \"F.Undergrad\",\"Outstate\",\"Terminal\",\"Top10perc\",\"Grad.Rate\",\"PhD\",\"Top25perc\",\"Expend\",\"Accept\"\n\n## ii. 17 terminal nodes \n\n## iii. MCR: 0.01762\n\n## iv. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(tree_model, C.test, type = \"class\")\ntable(C.test$Private, test_predictions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     test_predictions\n       No Yes\n  No  236   8\n  Yes   4 652\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(4+8)/(234 + 8 + 4 + 652)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01336303\n```\n\n\n:::\n:::\n\n\n\n\nThe testing MCR is 1.33%.\n\n# C. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot size vs deviance to visualize the optimal cp for pruning\ncv_tree <- cv.tree(tree_model, FUN = prune.misclass)\nplot(cv_tree$size, cv_tree$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\nSize of pruned tree: 15\n\n# D.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprune_model <- prune.misclass(tree_model, best = 15)\nplot(prune_model)\ntext(prune_model, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(prune_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nClassification tree:\nsnip.tree(tree = tree_model, nodes = c(52L, 12L))\nVariables actually used in tree construction:\n[1] \"F.Undergrad\" \"Outstate\"    \"Terminal\"    \"Top10perc\"   \"Grad.Rate\"  \n[6] \"PhD\"         \"Top25perc\"   \"Expend\"      \"Accept\"     \nNumber of terminal nodes:  15 \nResidual mean deviance:  0.1375 = 286.7 / 2085 \nMisclassification error rate: 0.0181 = 38 / 2100 \n```\n\n\n:::\n:::\n\n\n\n\nThe MCR of the pruned model is 18.1%.\n\n# E\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_predictions <- predict(prune_model, newdata = C.test,type = \"class\")\ntable(test_predictions, C.test$Private)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                \ntest_predictions  No Yes\n             No  236   7\n             Yes   8 649\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n(8 + 7)/(236 + 7 + 8 + 649)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01666667\n```\n\n\n:::\n:::\n\n\n\n\nThe MCR is 1.66%.\n\n# 2\n\n# A. \n\n## i. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(C.train$Expend)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3186    6863    8444    9727   10822   56233 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(C.test$Expend)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   3186    6704    8189    9477   10622   45702 \n```\n\n\n:::\n:::\n\n\n\n\n\n## ii.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(C.train,aes(x = Expend))+geom_density(fill = \"blue\", alpha = 0.5)+labs(title = \"Density Plot of Expend\",x = \"Expend\",y = \"Density\")+theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### iii.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC.train$Expend <-log(C.train$Expend)\nC.test$Expend <-log(C.test$Expend)\nggplot(C.train,aes(x = Expend))+geom_density(fill = \"blue\", alpha = 0.5)+labs(title = \"Density Plot of Log(Expend)\",x = \"Expend\",y = \"Density\")+theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n# B. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a tree model to predict log(Expend)\nlog_tree_model <- tree(Expend ~ ., data = C.train)\nsummary(log_tree_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRegression tree:\ntree(formula = Expend ~ ., data = C.train)\nVariables actually used in tree construction:\n[1] \"Outstate\"  \"S.F.Ratio\" \"Enroll\"    \"Top10perc\" \"Top25perc\"\nNumber of terminal nodes:  11 \nResidual mean deviance:  0.04209 = 87.93 / 2089 \nDistribution of residuals:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.6196 -0.1252  0.0000  0.0000  0.1086  1.1040 \n```\n\n\n:::\n:::\n\n\n\n\n\ni.5 predictors used for the model: \"Outstate\"  \"S.F.Ratio\" \"Enroll\"  \"Top10perc\" \"Top25perc\"\n\nii. 11 terminal nodes\n\niii. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_predictions <- predict(log_tree_model, data = C.train)\ntrain_residuals <- C.train$Expend - train_predictions\nmean(train_residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04187042\n```\n\n\n:::\n:::\n\n\n\n\n\nTraining MSE:  0.04187042\n\niv.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC.test$Private <- as.factor(C.test$Private)\ntest_predictions <- predict(log_tree_model, newdata = C.test)\ntest_residuals <- C.test$Expend - test_predictions\nmse_test <- mean(test_residuals^2)\nmse_test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04207581\n```\n\n\n:::\n:::\n\n\n\n\nTesting MSE: 0.04207581\n\n# C\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_tree_2 <- cv.tree(log_tree_model)\nplot(cv_tree_2$size, cv_tree_2$dev, type = \"b\")\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\nSize of pruned tree: 14\n\n# D\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Plot pruned tree model\nprune_log_model <- prune.tree(log_tree_model, best = 14)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in prune.tree(log_tree_model, best = 14): best is bigger than tree size\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(prune_log_model)\ntext(prune_log_model, pretty = 0)\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Training MSE of pruned tree\ntrain_predictions <- predict(prune_log_model, data = C.train)\ntrain_residuals <- C.train$Expend - train_predictions\nmean(train_residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04187042\n```\n\n\n:::\n:::\n\n\n\n\nTraining MSE of pruned tree model: 0.04187042\n\n# E\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Predict log(Expend) with testing data\ntest_predictions <- predict(prune_log_model, newdata = C.test)\ntest_residuals <- C.test$Expend - test_predictions\nmean(test_residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04207581\n```\n\n\n:::\n:::\n\n\n\n\nTesting MSE: 0.04207581\n\n# F\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(cor(train_predictions, C.train$Expend))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.751356\n```\n\n\n:::\n:::\n\n\n\n\nThe r squared of the training data of 75.14%\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(cor(test_predictions, C.test$Expend))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7460448\n```\n\n\n:::\n:::\n\n\n\n\nThe r squared of the testing data is 74.6%\n\n# 3\n\n# A\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(705780612)\n# Create a bagging model\nC.train$Private <- as.factor(C.train$Private)\nbagging <- randomForest(Private ~., data = C.train, mtry = 17, importance = TRUE)\nbagging\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Private ~ ., data = C.train, mtry = 17,      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 17\n\n        OOB estimate of  error rate: 0.81%\nConfusion matrix:\n     No  Yes class.error\nNo  496   11 0.021696252\nYes   6 1587 0.003766478\n```\n\n\n:::\n:::\n\n\n\n\n\n## i.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimportance(bagging)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   No      Yes MeanDecreaseAccuracy MeanDecreaseGini\nApps         7.996776 14.63382             16.29653         7.852547\nAccept      16.440909 16.77159             22.45017        18.458509\nEnroll       9.946075 10.29376             13.71551         9.608839\nTop10perc   24.656129 22.80333             29.19540        15.902975\nTop25perc   21.502638 25.66150             27.33541        13.640813\nF.Undergrad 81.217340 78.50145            105.98728       387.934372\nP.Undergrad 22.044252 34.70942             34.37470        28.152861\nOutstate    94.639180 97.70798            137.65480       200.661968\nRoom.Board  14.147645 38.00999             39.62783        15.609351\nBooks       15.064323 21.26806             20.72064         5.125512\nPersonal    11.723063 15.97299             18.08786         3.527622\nPhD         20.798156 18.54041             24.07538        13.790794\nTerminal    38.247797 23.37693             38.82201        19.796038\nS.F.Ratio   13.228533 15.97735             18.68794         5.975525\nperc.alumni 26.961270 18.60419             29.63970         7.436632\nGrad.Rate   18.230071 14.98763             20.89308         8.983862\nExpend      10.470543 15.27206             17.96734         5.633619\n```\n\n\n:::\n:::\n\n\n\n\n\n## ii.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importance\nvarImpPlot(bagging)\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n\nThe most importance variables is \"Outstate\".\n\n## iii.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Report testing MCR\npredictions <- predict(bagging, newdata = C.test)\nactual <- C.test$Private  \nmisclassification_rate <- mean(predictions != actual)\nmisclassification_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.001111111\n```\n\n\n:::\n:::\n\n\n\n\nThe testing MCR is 0.11%\n \n# B. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a random forest model with mtry = 4\nrf <- randomForest(Private ~., data = C.train, mtry = 4, importance = TRUE)\nrf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Private ~ ., data = C.train, mtry = 4,      importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n        OOB estimate of  error rate: 0.57%\nConfusion matrix:\n     No  Yes class.error\nNo  498    9 0.017751479\nYes   3 1590 0.001883239\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Report testing MCR\npredictions.2 <- predict(rf, newdata = C.test)\nmisclassification_rate <- mean(predictions.2 != C.test$Private)\nmisclassification_rate\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.003333333\n```\n\n\n:::\n:::\n\n\n\n\nThe testing MCR is 0.33%\n\n# 4\n\n# A.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a bagging model to predict logExpend\nlog_bagging <- randomForest(Expend ~., data = C.train, mtry = 17, importance = TRUE)\nlog_bagging\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Expend ~ ., data = C.train, mtry = 17,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 17\n\n          Mean of squared residuals: 0.003817121\n                    % Var explained: 97.73\n```\n\n\n:::\n:::\n\n\n\n\n\n## i.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Report important variables\nimportance(log_bagging)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              %IncMSE IncNodePurity\nPrivate       7.85475     0.1365468\nApps         29.29824     4.1939672\nAccept       17.38036     2.9655459\nEnroll       30.21717     3.5845051\nTop10perc    70.60875    28.9734666\nTop25perc    22.16605     7.2507768\nF.Undergrad  27.89312     3.6979415\nP.Undergrad  59.33170     7.0736315\nOutstate    120.41569   193.0642281\nRoom.Board   32.13347    10.0754965\nBooks        40.52761     3.8407706\nPersonal     42.15975     4.6976768\nPhD          49.44477    14.2506218\nTerminal     38.25795     7.3654831\nS.F.Ratio   138.11138    51.6618400\nperc.alumni  47.62025     5.4791384\nGrad.Rate    25.96474     4.8544786\n```\n\n\n:::\n:::\n\n\n\n\n\n## ii.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot variable importance\nvarImpPlot(log_bagging)\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\n\nThe most importance predictor based on %IncMSE is S.F ratio\n\n## iii.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predict log(Expend) with testing data \ntest_predictions.2 <- predict(log_bagging, newdata = C.test)\ntest_residuals <- C.test$Expend - test_predictions.2\nmean(test_residuals^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002179744\n```\n\n\n:::\n:::\n\n\n\n\nThe testing MSE 0.218%\n\n## iv.\nThe R squared of the model is 97.73%\n\n# B\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_bagging_4 <-randomForest(Expend~., data = C.train, mtry = 4, importance = TRUE)\nlog_bagging_4\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n randomForest(formula = Expend ~ ., data = C.train, mtry = 4,      importance = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 4\n\n          Mean of squared residuals: 0.003541376\n                    % Var explained: 97.9\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions.3 <- predict(log_bagging_4, newdata = C.test)\nmean((predictions.3 - C.test$Expend)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.002079963\n```\n\n\n:::\n:::\n\n\n\n\nThe MCR is 0.21%\n\n# C\nThe R squared is 97.9%\n\n# D. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC.test$pred <- predictions.3\nggplot(C.test,aes(x = Expend, y = predictions.3))+geom_point(color = \"green\", alpha = 0.6)+ # Scatter plot\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") + # 45-degree line\n  labs(title = \"Actual vs Predicted log(Expend)\",x = \"Actual log(Expend)\",y = \"Predicted log(Expend)\")+\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](01-project_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n\nThe plot shows a linear relationship in the actual and the predicted data, and the predicted data matches the actual data really well.",
    "supporting": [
      "01-project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}